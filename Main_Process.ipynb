{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the main function for Emotion Forecasting Project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to have the audio-visual features from the IEMOCAP dataset. We have the facial-video features available from IEMOCAP dataset. But we need to extract audio features. The skeleton code `audio_feat.praat` does the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to call different function from our classes to prepare the dataset from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Codes.Combine_audiovisual_data import combining_AV\n",
    "from Codes.window_based_reformation import window_based_reformation\n",
    "from Codes.Utt_Fore_Data_Prep import Prepare_UF_Cur_Data, Prepare_UF_history_Data\n",
    "from Codes.run_algorithms import run_sequential_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will combine the audio-visual features.\n",
    "\n",
    "The audio and visual features are not extracted in a same framerate. We have to do the following things:\n",
    "1. remove the `nan` features from video datasets\n",
    "2. Downsample the video features to make it as a same length as the audio features.\n",
    "\n",
    "The following code will do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combining_data = combining_AV('Files/audio_features', 'Files/video_features')\n",
    "Combining_data.produce_speakerwise_AV_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to use the sequential information of the audio-visual cues, we will create overlapped frames. These frames will have statistical information which will be used as features. So, the following code will do the tasks:\n",
    "\n",
    "1. Create window based sequences\n",
    "2. Find mean, standard deviation, first and third quantile, interquantile range of those windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Windowing = window_based_reformation('Files/sameframe')\n",
    "Windowing.process_data(window_type='dynamic') \n",
    "#important note: setting the window_type as 'static' will stop creating any window based features and create statistical features\n",
    "#from the whole sequence. We will use them to make a FC-DNN based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we prepare the dataset for utterance forecasting. The code is so designed that you can have any `step` of utterance forecasting. The code does the followign task:\n",
    "\n",
    "1. Process the `IEMOCAP_EmoEvaluation.txt` file and produces a smart look-up table for finding the time-distances for Utterance Forecasting.\n",
    "2. Create the dataset, normalize (or without normalize) it, and add zero padding at the end. The utterance length have variations and thus, the functions add zero (zero-padding) to make it of same length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "UF_cur = Prepare_UF_Cur_Data()\n",
    "UF_cur.creating_dataset(step=1, normalization=True)\n",
    "#Set any number of step. set normalization 'False' to not normalize the file. \n",
    "#If set 'True', it will produce speaker-wise z-normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create Utterance forecasting dataset with `History` information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "UF_his = Prepare_UF_history_Data()\n",
    "UF_his.creating_dataset(step=1, normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For running the sequential models, we use `LSTM` or `BLSTM`. Next chunk of code will create a model and run the forecasting task.\n",
    "The code serves following task:\n",
    "\n",
    "1. Prepare the feature matrix , label vector for emotion forecasting task using LSTM or BLSTM.\n",
    "2. If the model_type is selected as `unidirectional`, we will have regular LSTM cell, if set to `bidirectional`, BLSTM layers will be set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = run_sequential_learning()\n",
    "features, label, speaker_group = forecast.prepare_data(directory='Files/UF_His_data/step_1') #Change the file location accordingly\n",
    "forecast.LSTM(features, label, speaker_group, model_type='bidirectional') #if model_type set to unidirectional, it will be LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
